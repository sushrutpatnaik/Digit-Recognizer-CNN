{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    1000\n",
       "6    1000\n",
       "5    1000\n",
       "4    1000\n",
       "3    1000\n",
       "2    1000\n",
       "9    1000\n",
       "1    1000\n",
       "8    1000\n",
       "0    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset and reduce the dataset size\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "res = []\n",
    "for i in range(10):\n",
    "    res.append(train_data.loc[train_data['label'] == i, :].sample(n=1000))\n",
    "    \n",
    "result = pd.concat(res)\n",
    "result['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Y = result.label.values\n",
    "X = result.drop(\"label\", axis = 1).values.reshape((10000, 1, 28, 28)).astype(\"float32\") / 255\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 10, stratify = Y)\n",
    "\n",
    "x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset (x_test, y_test), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(7*7*64,64)\n",
    "        self.fc2 = nn.Linear(64,10)\n",
    "        self.sm = nn.Softmax(dim=None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=20, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sushrut/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is: 0\n",
      "Training loss is: 2.0140453033447265\n",
      "Epoch is: 1\n",
      "Training loss is: 1.6274664011001587\n",
      "Epoch is: 2\n",
      "Training loss is: 1.569475664138794\n",
      "Epoch is: 3\n",
      "Training loss is: 1.5502930068969727\n",
      "Epoch is: 4\n",
      "Training loss is: 1.538114369392395\n",
      "Epoch is: 5\n",
      "Training loss is: 1.5277278833389283\n",
      "Epoch is: 6\n",
      "Training loss is: 1.5198277416229249\n",
      "Epoch is: 7\n",
      "Training loss is: 1.5159213638305664\n",
      "Epoch is: 8\n",
      "Training loss is: 1.507336217880249\n",
      "Epoch is: 9\n",
      "Training loss is: 1.502874108314514\n",
      "Epoch is: 10\n",
      "Training loss is: 1.5006384840011597\n",
      "Epoch is: 11\n",
      "Training loss is: 1.495161029815674\n",
      "Epoch is: 12\n",
      "Training loss is: 1.4923762416839599\n",
      "Epoch is: 13\n",
      "Training loss is: 1.4906935577392577\n",
      "Epoch   230: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch is: 14\n",
      "Training loss is: 1.4881203746795655\n",
      "Epoch   251: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch is: 15\n",
      "Training loss is: 1.4846615924835205\n",
      "Epoch is: 16\n",
      "Training loss is: 1.483572135925293\n",
      "Epoch   272: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch is: 17\n",
      "Training loss is: 1.4824762706756591\n",
      "Epoch is: 18\n",
      "Training loss is: 1.4820121726989746\n",
      "Epoch   313: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch is: 19\n",
      "Training loss is: 1.481741792678833\n",
      "Epoch   334: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch is: 20\n",
      "Training loss is: 1.48157453250885\n",
      "Epoch is: 21\n",
      "Training loss is: 1.4813748111724854\n",
      "Epoch   355: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch is: 22\n",
      "Training loss is: 1.4813100337982177\n",
      "Epoch is: 23\n",
      "Training loss is: 1.4812639245986938\n",
      "Epoch   392: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch is: 24\n",
      "Training loss is: 1.4812271213531494\n",
      "Epoch   413: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch is: 25\n",
      "Training loss is: 1.4811980056762695\n",
      "Epoch is: 26\n",
      "Training loss is: 1.4811798582077027\n",
      "Epoch   434: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch is: 27\n",
      "Training loss is: 1.4811702919006349\n",
      "Epoch   455: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch is: 28\n",
      "Training loss is: 1.48116614818573\n",
      "Epoch is: 29\n",
      "Training loss is: 1.4811623821258544\n"
     ]
    }
   ],
   "source": [
    "# Training for 30 epochs\n",
    "\n",
    "n_epochs = 30\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "    print('Epoch is:', epoch)\n",
    "    print('Training loss is:', train_loss/len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sushrut/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.4999643449783324\n",
      "Overall Test Accuracy: 0.963\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = list(0.0 for i in range(10))\n",
    "class_total = list(0.0 for i in range(10))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for data, target in test_loader:\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item() * data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    \n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "        \n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('Overall Test Accuracy:', (np.sum(class_correct)) / np.sum(class_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
